---
layout: post
title: "스파크 완벽 가이드 - 구조적 API 개요" 
author: Gunju Ko
categories: [hadoop, spark]
cover:  "/assets/instacode.png"
---

> 이 글은 "스파크 완벽 가이드" 책 내용을 정리한 글입니다. 
>
> 저작권에 문제가 있는 경우 "gunjuko92@gmail.com"으로 연락주시면 감사하겠습니다.

## 4. 구조적 API 개요

구조적 API에는 다음과 같은 세 가지 분산 컬렉션 API가 있다.

* Dataset
* DataFrame
* SQL 테이블과 뷰

> 스파크의 개본 개념과 정의
>
> * 스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델이다.
> * 트랜스포메이션은 DAG로 표현되는 명령을 만들어낸다.
> * 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행한다.
> * 트랜스포메이션과 액션을 다루는 논리적 구조가 바로 DataFrame과 Dataset이다.

### 1. DataFrame과 Dataset

* DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션이다.
* DataFrame과 Dataset은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야하는지 정의하는 지연 연산의 실행 계획이며, 불변성을 가진다.

> 기본적으로 테이블과 뷰는 DataFrame과 같다. 대신 테이블은 SQL을 사용한다.

### 2. 스키마

* 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의한다.
* 스키마는 데이터소스에서 얻거나 직접 정의할 수 있다. 

### 3. 스파크의 구조적 데이터 타입 개요

* 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용한다.
* 카탈리스트 엔진은 다양한 실행 최적화 기능을 제공한다.
* 스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있다.
* 파이썬을 이용해 스파크의 구조적 API을 사용하더라도 대부분의 연산은 파이썬 데이터 타입이 아닌 스파크의 데이터 타입을 사용한다.
* 결과적으로 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있다.

#### 3.1 DataFrame과 Dataset 비교

* DataFrame : 스키마에 명시된 데이터 타입의 일치 여부를 런타임에 확인
* Dataset : 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인
* Dataset은 스칼라와 자바에서만 지원
* Dataset의 데이터 타입을 정의하려면 스칼라의 케이스 클래스(case class)나 자바빈(JavaBean)을 사용
* DataFrame은 Row 타입으로 구성된 Dataset이다.

> Row 타입
>
> * 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식
> * 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산이 가능

#### 3.2 컬럼

* 컬럼은 단순 데이터 타입(정수, 문자열 등), 복합 데이터 타입(배열, 맵 등), 널값을 표한한다.
* 스파크는 데이터 타입의 모든 정보를 추적하며 다양한 컬럼 변환 방법을 제공한다.
* 테이블의 컬럼으로 생각할 수 있다.

#### 3.3 로우

* 로우는 데이터 레코드이다.
* DataFrame의 레코드는 Row 타입으로 구성된다.
* 로우는 SQL, RDD, 데이터소스에서 얻거나 직접 생성할 수 있다.

``` scala
spark.range(2).toDF().collect()
```

#### 3.4 스파크 데이터 타입

* 스파크 데이터 타입을 스칼라에서 사용하려면 아래와 같은 코드를 사용한다.

``` scala
import org.apache.spark.sql.types._

val b = ByteType
```

* 스파크가 지원하는 언어별 매핑 정보는 [공식문서 - Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) 를 참고하길 바란다.

### 4. 구조적 API의 실행 과정

1. 코드 작성
2. 스파크가 코드를 논리적 실행 계획으로 변환
3. 스파크가 논리적 실행 계획을 물리적 실행 계획으로 변환 + 최적화 진행
4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행

> 카탈리스트 옵티마이저는 코드를 넘겨받고 실행 계획을 생성하는 역할을 한다.

#### 4.1 논리적 실행 계획

* 추상적 트랜스포메이션만 표현
* 드라이버나 익스큐터의 정보를 고려하지 않음
* 스파크 분석기는 컬럼과 테이블 검증을 위해 카탈로그, 모든 테이블 저장소, DataFrame 정보를 활용

![그림]({{ site.url }}/assets/img/posts/spark-guide/chapter4/photo1.png)

* 검증 전 논리적 실행 계획 : 유효성과 테이블이나 컬럼의 존재 여부만 검증된 상태 (실행 계획을 검증하지 않은 상태)
* 필요한 테이블이나 컬럼이 카탈로그에 없는 경우 검증전 논리적 실행 계획이 만들어지지 않음
* 테이블과 컬럼에 대한 검증 결과는 옵티마이저로 전달
* 카탈로그 옵티마이저는 논리적 실행 계획을 최적화하는 규칙의 모음이다.

#### 4.2 물리적 실행 계획 (스파크 실행 계획)

* 물리적 실행 계획은 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의
* 다양한 물리적 실행 계획 전략을 생성하고 비용 모델을 고려해서 비교한 후 최적의 전략을 선택
* 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됌
* 스파크는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일한다.

![그림]({{ site.url }}/assets/img/posts/spark-guide/chapter4/photo2.png)

#### 4.3 실행

* 일련의 RDD와 트랜스포메이션으로 모든 코드를 실행한다.
* 처리 결과를 사용자에게 반환한다.

